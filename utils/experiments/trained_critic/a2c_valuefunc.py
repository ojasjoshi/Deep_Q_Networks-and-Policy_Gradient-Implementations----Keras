import sys
import argparse
import numpy as np
import keras
import random
import gym
from keras.utils import plot_model
from keras.optimizers import Adam
from keras.layers import Dense, Activation, Dropout, Input, Lambda, Add, Subtract
from keras.models import Sequential, Model
from keras import backend as K


def critic_loss(y_true, y_pred):                                                                                                                     # referenced from tensorflow sourcecode
  return K.mean(K.square(y_pred - y_true), axis=-1)*1e-2

class Imitation():
    def __init__(self, value_model, model_config_path, expert_weights_path):
        
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)

        self.learning_rate = 0.0001
        self.n = 20
        self.value_model = value_model
        self.value_model.compile(optimizer = Adam(lr=self.learning_rate), loss=critic_loss)

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.value_model, env, render)

    def V_t(self,states):
        val_preds = [self.value_model.predict(states[t])[0] for t in range(len(states))]
        return val_preds
    
    def R_t_util(self, rewards, t, T, gamma=1.0):
        ret = 0
        for k in range(self.n):
            if(t+k<T):
                ret += pow(gamma,k)*rewards[t+k]
        return ret

    @staticmethod
    def make_one_hot(env, action):
        one_hot_action_vector = np.zeros(env.action_space.n)
        one_hot_action_vector[action] = 1
        return one_hot_action_vector

    @staticmethod
    def G_t(rewards,gamma=1):       
        updated_rewards = [0]
        current_index = 1
        for t in range(len(rewards)-1,-1,-1):
            gt = rewards[t] + gamma*updated_rewards[current_index-1]
            # updated_rewards.append(gt/len(rewards))                                                                               #potential problem
            updated_rewards.append(gt)
            current_index +=1
        return list(reversed(updated_rewards[1:]))

    def R_t(self, rewards, value_preds, gamma=1.0):                                                                                                    # not static because need 'n'
        updated_rewards = []
        T = len(rewards)
        N = self.n
        for t in range(T-1,-1,-1):
            V_end = 0
            if(N+t<T):
                V_end = value_preds[t+N]
            rt = pow(gamma,N)*V_end + self.R_t_util(rewards,t,T,gamma)
            updated_rewards.append(rt)
        return list(reversed(updated_rewards))

    @staticmethod
    def generate_episode(model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.
        states = []
        actions = []
        rewards = []

        state = env.reset()
        action = np.argmax(model.predict(state.reshape([1,env.observation_space.shape[0]])))
        while(True):
            if(render==True):
                env.render()
            state = state.reshape([1,env.observation_space.shape[0]])
            states.append(state)                                            #storing reshaped state
            actions.append(Imitation.make_one_hot(env,action))              #storing one hot action target

            nextstate, reward, is_terminal, _ = env.step(action)
    
            rewards.append(reward/200)                                          #storing reward
            if(is_terminal == True):
                break                            
            state = nextstate.reshape([1,env.observation_space.shape[0]])
            action = np.argmax(model.predict(state))
        return states, actions, rewards
    
    def train(self, env, num_episodes=10000, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        
        loss = 0
        acc = 0

        current_episode = 0
        while(current_episode<num_episodes):
            #Generate episode as current training batch
            states,actions, rewards = self.run_expert(env,False)
            current_batch_size = len(states)

            value_predictions = self.V_t(states)
            updated_rewards = self.R_t(rewards,value_predictions)
            monte_carlo_rewards = Imitation.G_t(rewards)
            rew_dif_sum = np.sum([abs(a_i - b_i) for a_i, b_i in zip(monte_carlo_rewards, value_predictions)])/current_batch_size
            rew_dif_sum_norm = np.sum([abs((a_i - b_i)/a_i) for a_i, b_i in zip(monte_carlo_rewards, value_predictions)])/current_batch_size

            history = self.value_model.fit(np.vstack(states),np.asarray(updated_rewards),epochs=1,verbose=0,batch_size=current_batch_size)
            if(current_episode%100==0):
                print("Episodes: {}, Loss: {}, Value_Prediction_bias: {}, Value_Prediction_bias_norm: {}".format(current_episode, history.history['loss'], rew_dif_sum, rew_dif_sum_norm))
            self.value_model.save('backup')
            current_episode += 1

        self.value_model.save('backup')

    def test(self, env, num_episodes=50, render=False):
        current_episode = 0
        rewards = []
        while(current_episode<num_episodes):
            if(render==True):
                env.render()
            _,_,r = self.run_model(env,render)
            rewards.append(r)
            current_episode +=1

        return np.std(np.hstack(rewards)),np.mean(np.hstack(rewards))


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    return parser.parse_args()


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    
    # Create the environment.
    env = gym.make('LunarLander-v2')
    
    # Critic Model
    inp = Input(shape=(env.observation_space.shape[0],))
    layer_dense = Dense(16,activation='relu',kernel_initializer='he_uniform',use_bias = True)(inp)
    layer_dense = Dense(16,activation='relu',kernel_initializer='he_uniform',use_bias = True)(layer_dense)
    layer_dense = Dense(16,activation='relu',kernel_initializer='he_uniform',use_bias = True)(layer_dense)
    layer_v = Dense(1,activation='linear',kernel_initializer='he_uniform',use_bias = True)(layer_dense)
    critic_model = Model(inp, layer_v)

    imitating_agent = Imitation(critic_model, 'LunarLander-v2-config.json','LunarLander-v2-weights.h5')
    acc = imitating_agent.train(env,10000)

    # Test cloned policy
    # std,mean = imitating_agent.test(env,render=args.render)
    # file_test = open('behaviour_cloning/accuracy.txt', 'a+')
    # file_test.write("\n"+str(episodes)+" "+str(acc)+" "+"std: "+str(std)+" mean: "+str(mean))


if __name__ == '__main__':
  main(sys.argv)
 